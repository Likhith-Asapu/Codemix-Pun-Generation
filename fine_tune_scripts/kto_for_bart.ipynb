{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers import MBartForConditionalGeneration, AutoModelForSeq2SeqLM\n",
    "from transformers import AlbertTokenizer, AlbertForMaskedLM\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, BartForCausalLM\n",
    "import torch\n",
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq\n",
    "from torch.utils.data import DataLoader\n",
    "from trl import KTOConfig, KTOTrainer, ModelConfig, get_peft_config, setup_chat_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home2/likhithasapu/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "You are using a model of type mbart to instantiate a model of type bart. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at likhithasapu/codemix-indicbart and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are using a model of type mbart to instantiate a model of type bart. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at likhithasapu/codemix-indicbart and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"likhithasapu/codemix-indicbart\")\n",
    "# tokenizer = AlbertTokenizer.from_pretrained(\"ai4bharat/IndicBART\", do_lower_case=False, use_fast=False, keep_accents=True)\n",
    "\n",
    "model = BartForCausalLM.from_pretrained(\"likhithasapu/codemix-indicbart\")\n",
    "model_ref = BartForCausalLM.from_pretrained(\"likhithasapu/codemix-indicbart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.chat_template is None:\n",
    "    model, tokenizer = setup_chat_format(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/likhithasapu/miniconda3/envs/research/lib/python3.11/site-packages/datasets/load.py:2069: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=True' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['data.idx', 'data.L1', 'data.L2', 'data.alignments', 'data.CM_candidates', 'data.CM_candidates_transliterated_indictrans', 'average_rating', 'int_annotations', 'LID', 'PoSTags'],\n",
      "        num_rows: 2145\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['data.idx', 'data.L1', 'data.L2', 'data.alignments', 'data.CM_candidates', 'data.CM_candidates_transliterated_indictrans', 'average_rating', 'int_annotations', 'LID', 'PoSTags'],\n",
      "        num_rows: 7507\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['data.idx', 'data.L1', 'data.L2', 'data.alignments', 'data.CM_candidates', 'data.CM_candidates_transliterated_indictrans', 'average_rating', 'int_annotations', 'LID', 'PoSTags'],\n",
      "        num_rows: 1073\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"likhithasapu/codemix-annotated-dataset\",use_auth_token=True,cache_dir=\"/scratch/annotated-codemix-dataset\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'completion', 'label'],\n",
      "        num_rows: 2145\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'completion', 'label'],\n",
      "        num_rows: 7507\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['prompt', 'completion', 'label'],\n",
      "        num_rows: 1073\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    prompt = [{\"role\": \"user\", \"content\": f\"Translate the English sentence to Hindi-English sentence: <s> {example} </s>\"} for example in examples['data.L2']]\n",
    "    completion = [{\"role\": \"assistant\", \"content\": f\"<s> {example} </s>\"} for example in examples['data.CM_candidates']]\n",
    "    \n",
    "    label = [bool(example > 3) for example in examples['average_rating']]\n",
    "    \n",
    "    return{\n",
    "        \"prompt\": prompt,\n",
    "        \"completion\": completion,\n",
    "        \"label\": label\n",
    "    \n",
    "    }\n",
    "    \n",
    "# Apply the preprocess function to the dataset with batching\n",
    "batch_size = 64\n",
    "processed_data = data.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    remove_columns=data[\"train\"].column_names\n",
    ")\n",
    "print(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/2145 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2145/2145 [00:00<00:00, 5799.76 examples/s]\n",
      "Map: 100%|██████████| 7507/7507 [00:01<00:00, 7127.01 examples/s]\n",
      "Map: 100%|██████████| 1073/1073 [00:00<00:00, 7050.10 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'completion', 'label'],\n",
      "        num_rows: 2145\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'completion', 'label'],\n",
      "        num_rows: 7507\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['prompt', 'completion', 'label'],\n",
      "        num_rows: 1073\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply chat template\n",
    "def format_dataset(example):\n",
    "    example[\"prompt\"] = tokenizer.apply_chat_template([example[\"prompt\"]], tokenize=False)\n",
    "    example[\"completion\"] = tokenizer.apply_chat_template([example[\"completion\"]], tokenize=False)\n",
    "    return example\n",
    "\n",
    "processed_data = processed_data.map(format_dataset)\n",
    "print(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': '<|im_start|>user\\nTranslate the English sentence to Hindi-English sentence: <s> A lot still needs to be done in this regard . </s><|im_end|>\\n',\n",
       " 'completion': '<|im_start|>assistant\\n<s> A lot still needs to be किया इस मामले में . </s><|im_end|>\\n',\n",
       " 'label': False}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing train dataset:   0%|          | 0/7507 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing train dataset: 100%|██████████| 7507/7507 [00:01<00:00, 5878.82 examples/s]\n",
      "Extracting KL train dataset: 100%|██████████| 7507/7507 [00:00<00:00, 15752.36 examples/s]\n",
      "Processing tokenized train dataset: 100%|██████████| 7507/7507 [00:01<00:00, 4521.94 examples/s]\n",
      "Processing tokenized train KL dataset: 100%|██████████| 7507/7507 [00:01<00:00, 4776.38 examples/s]\n",
      "Filtering desirable examples: 100%|██████████| 7507/7507 [00:01<00:00, 4017.18 examples/s]\n",
      "Filtering undesirable examples: 100%|██████████| 7507/7507 [00:02<00:00, 3707.46 examples/s]\n",
      "/home2/likhithasapu/miniconda3/envs/research/lib/python3.11/site-packages/trl/trainer/kto_trainer.py:598: UserWarning: \n",
      "                        You have different amounts of desirable/positive and undesirable/negative examples but the\n",
      "                        weights on the desirable and undesirable losses don't seem to be in an ideal range. Based\n",
      "                        on your data, we recommend EITHER desirable_weight in [0.67, 0.9]\n",
      "                        or undesirable_weight in [1.12, 1.48] (but NOT BOTH).\n",
      "                        See the documentation on how to optimally set these weights.\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "training_args = KTOConfig(\n",
    "    beta=0.1,\n",
    "    desirable_weight=1.0,\n",
    "    undesirable_weight=1.0,\n",
    "    output_dir=\"/scratch/likhithasapu/output\",\n",
    "    max_completion_length=128,\n",
    "    max_prompt_length=128,\n",
    "    is_encoder_decoder=True,\n",
    "    max_length=128,\n",
    ")\n",
    "\n",
    "kto_trainer = KTOTrainer(\n",
    "    model,\n",
    "    model_ref,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_data[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This एक transformers का model card है. \n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"likhithasapu/codemix-indicbart\")\n",
    "# tokenizer = AlbertTokenizer.from_pretrained(\"ai4bharat/IndicBART\", do_lower_case=False, use_fast=False, keep_accents=True)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"likhithasapu/codemix-indicbart\")\n",
    "\n",
    "# Prepare the text\n",
    "text = \"Translate the English sentence to Hindi-English sentence: <s> This is the model card of a transformers </s>\"\n",
    "\n",
    "# Encode the text\n",
    "inputs = tokenizer.encode(text, return_tensors='pt').to(model.device)\n",
    "\n",
    "# Generate a response\n",
    "outputs = model.generate(inputs, max_length=50, num_beams=5, early_stopping=True)\n",
    "\n",
    "# Decode the response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

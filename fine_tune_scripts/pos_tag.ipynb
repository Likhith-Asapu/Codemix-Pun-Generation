{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/likhithasapu/miniconda3/envs/research/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers import MBartForConditionalGeneration, AutoModelForSeq2SeqLM\n",
    "from transformers import AlbertTokenizer, AlbertForMaskedLM\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "from transformers import AutoModelForTokenClassification, DataCollatorForSeq2Seq\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "# give code to generate wordwise pos tags using finetuned model \"likhithasapu/gcm-xlmr-pos\" present on huggingface\n",
    "\n",
    "# Load the pre-trained model\n",
    "model_name = \"likhithasapu/gcm-xlmr-pos\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Create a pipeline\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer,aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PRON', 'score': 0.69416314, 'word': 'वह', 'start': 0, 'end': 2}, {'entity_group': 'NOUN', 'score': 0.8833633, 'word': 'dance practice', 'start': 3, 'end': 17}, {'entity_group': 'VERB', 'score': 0.99882716, 'word': 'कर रही थी', 'start': 18, 'end': 27}, {'entity_group': 'CONJ', 'score': 0.3799433, 'word': ',', 'start': 27, 'end': 28}, {'entity_group': 'ADV', 'score': 0.999087, 'word': 'now', 'start': 29, 'end': 32}, {'entity_group': 'PRON', 'score': 0.99904853, 'word': 'she', 'start': 33, 'end': 36}, {'entity_group': 'VERB', 'score': 0.9996284, 'word': 'has', 'start': 37, 'end': 40}, {'entity_group': 'PART', 'score': 0.99955493, 'word': 'to', 'start': 41, 'end': 43}, {'entity_group': 'VERB', 'score': 0.9059099, 'word': 'speed', 'start': 44, 'end': 49}, {'entity_group': 'ADV', 'score': 0.9967789, 'word': 'अब', 'start': 50, 'end': 52}, {'entity_group': 'X', 'score': 0.99468935, 'word': '.', 'start': 52, 'end': 53}]\n",
      "अब: ADV : 0.9967789053916931\n"
     ]
    }
   ],
   "source": [
    "sentence = \"वह dance practice कर रही थी, now she has to speed अब.\"\n",
    "pun_word = \"अब\"\n",
    "\n",
    "# Generate wordwise POS tags\n",
    "result = nlp(sentence)\n",
    "\n",
    "# Print the word with its POS tag\n",
    "print(result)\n",
    "for word in result:\n",
    "    if pun_word == word['word']:\n",
    "        print(f\"{word['word']}: {word['entity_group']} : {word['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read from the input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:40<00:00, 12.46it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "file_name = \"/home2/likhithasapu/Codemixed-Pun-Generation/pun/automated_gen/pun_4.json\"\n",
    "\n",
    "new_data = []\n",
    "\n",
    "with open(file_name, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "for row in tqdm(data, total=len(data)):\n",
    "    sentences = row[\"Candidates\"]\n",
    "    pun_word = row[\"Pun_word\"]\n",
    "    pun_word_pos = row[\"Pun_word_pos\"]\n",
    "    new_sentences = []\n",
    "    weights = []\n",
    "    if pun_word_pos == 'X':\n",
    "        new_data.append(row)\n",
    "        continue\n",
    "    for sentence in sentences:\n",
    "        result = nlp(sentence)\n",
    "        # iterate in reverse order and store the relative postion in weights\n",
    "        for index, word in enumerate(reversed(result)):\n",
    "            if pun_word == word['word']:\n",
    "                if word['entity_group'] == pun_word_pos:\n",
    "                    new_sentences.append(sentence)\n",
    "                    weights.append((len(result) - index)/len(result))\n",
    "                    break\n",
    "    if len(new_sentences) > 0:\n",
    "        row[\"Candidates\"] = new_sentences\n",
    "        # Sample random sentence based on weights\n",
    "        row[\"Sentence_chosen\"] = random.choices(new_sentences, weights=weights)[0]\n",
    "    new_data.append(row)\n",
    "    \n",
    "with open(\"/home2/likhithasapu/Codemixed-Pun-Generation/pun/automated_gen/pun_4_pos.json\", \"w\") as file:\n",
    "    json_data = json.dumps(new_data, indent=4, ensure_ascii=False)\n",
    "    file.write(json_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ब: NOUN\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained model\n",
    "model_name = \"likhithasapu/gcm-xlmr-pos\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, is_split_into_words=True)\n",
    "\n",
    "normal_word = \"बडी\"\n",
    "\n",
    "# Test sentence\n",
    "normal_sentence = \"मेरे big project के लिए, मैंने एक loyal बडी की मदद ली।\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "normal_inputs = tokenizer(normal_sentence, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Get the model's output\n",
    "normal_outputs = model(**normal_inputs)\n",
    "\n",
    "# Get the predicted tags\n",
    "normal_predictions = torch.argmax(normal_outputs.logits, dim=-1)\n",
    "\n",
    "# Encode the word\n",
    "encoded_word = tokenizer.encode(normal_word)[1]\n",
    "            \n",
    "# Print the word with its POS tag\n",
    "pred = 0\n",
    "for token, prediction in zip(normal_inputs['input_ids'][0], normal_predictions[0]):\n",
    "    if token == encoded_word:\n",
    "        pred = prediction\n",
    "        print(f\"{tokenizer.decode([token])}: {model.config.id2label[int(prediction)]}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[8.2411e-02, 8.6734e-02, 1.0558e-01, 6.2124e-02, 1.2014e-01,\n",
       "          5.2085e-02, 1.0547e-01, 4.4481e-02, 6.1834e-02, 5.5296e-02,\n",
       "          2.8173e-02, 3.0145e-02, 5.0588e-02, 6.3527e-02, 5.1405e-02],\n",
       "         [3.9167e-04, 5.3215e-04, 2.5284e-03, 6.4683e-04, 5.0639e-03,\n",
       "          9.8368e-01, 4.3724e-03, 2.6216e-04, 4.6929e-04, 2.9409e-04,\n",
       "          4.0613e-04, 2.2633e-04, 2.3247e-04, 5.5494e-04, 3.3960e-04],\n",
       "         [5.1534e-05, 1.2726e-04, 1.4751e-04, 1.4069e-04, 9.9854e-01,\n",
       "          6.4347e-05, 2.5797e-04, 9.6940e-05, 8.7590e-05, 5.6560e-05,\n",
       "          4.2024e-05, 9.1498e-05, 1.1887e-04, 8.5397e-05, 9.3892e-05],\n",
       "         [1.4340e-05, 9.1021e-05, 3.2202e-05, 9.0232e-05, 2.0917e-05,\n",
       "          4.4350e-05, 1.6770e-04, 7.7392e-05, 6.8204e-05, 9.9899e-01,\n",
       "          7.3494e-05, 1.2731e-04, 1.2671e-04, 4.9806e-05, 2.5499e-05],\n",
       "         [1.3029e-04, 1.8718e-04, 1.5549e-03, 1.2750e-04, 9.9676e-01,\n",
       "          1.2446e-04, 4.3178e-04, 7.5850e-05, 7.3311e-05, 9.5747e-05,\n",
       "          3.6089e-05, 1.5603e-04, 6.9479e-05, 9.8927e-05, 8.2898e-05],\n",
       "         [1.2808e-04, 3.5095e-04, 2.2414e-04, 2.1061e-04, 9.9643e-01,\n",
       "          7.6736e-05, 6.8219e-04, 1.2132e-04, 1.6138e-04, 2.6683e-04,\n",
       "          4.4629e-05, 6.3724e-04, 3.1349e-04, 2.0039e-04, 1.5317e-04],\n",
       "         [1.0811e-04, 3.0323e-04, 3.3528e-04, 1.2549e-04, 7.2888e-05,\n",
       "          9.0868e-05, 5.7568e-04, 2.2340e-04, 2.6097e-04, 9.9723e-01,\n",
       "          1.0530e-04, 2.1775e-04, 4.6678e-05, 2.0571e-04, 9.4979e-05],\n",
       "         [1.6796e-04, 1.0133e-04, 3.1199e-04, 7.0380e-05, 9.9838e-01,\n",
       "          5.6550e-05, 3.7662e-04, 7.8265e-05, 7.9866e-05, 6.5837e-05,\n",
       "          2.9066e-05, 8.5356e-05, 6.2367e-05, 5.9838e-05, 7.3864e-05],\n",
       "         [1.4455e-04, 3.6119e-04, 1.3275e-04, 6.0761e-04, 9.9581e-01,\n",
       "          1.5236e-04, 5.6690e-04, 2.2720e-04, 2.1553e-04, 2.4597e-04,\n",
       "          5.4320e-05, 8.1380e-04, 2.4014e-04, 2.6212e-04, 1.6276e-04],\n",
       "         [1.5685e-04, 6.4880e-05, 1.9704e-04, 4.5121e-05, 1.4458e-04,\n",
       "          7.8218e-05, 9.9875e-01, 8.0406e-05, 7.6639e-05, 1.2343e-04,\n",
       "          2.4141e-05, 5.0795e-05, 8.9164e-05, 6.9044e-05, 4.6613e-05],\n",
       "         [9.5271e-03, 9.4366e-04, 1.5463e-03, 1.0714e-03, 8.1129e-04,\n",
       "          2.7662e-02, 2.0373e-03, 1.5683e-03, 2.3859e-03, 9.9898e-04,\n",
       "          9.4817e-01, 3.5618e-04, 9.5200e-04, 8.4779e-04, 1.1219e-03],\n",
       "         [1.3365e-04, 8.9428e-05, 5.8599e-05, 1.6272e-04, 3.1015e-05,\n",
       "          8.0137e-04, 8.0030e-05, 1.6328e-04, 1.4814e-04, 2.0691e-04,\n",
       "          9.9782e-01, 4.3614e-05, 9.5986e-05, 7.3307e-05, 9.0355e-05],\n",
       "         [4.7565e-04, 3.8134e-04, 1.8653e-03, 4.3371e-04, 2.2092e-03,\n",
       "          9.8809e-01, 3.3385e-03, 3.5138e-04, 5.1598e-04, 3.8507e-04,\n",
       "          5.2026e-04, 1.9718e-04, 2.1344e-04, 5.6313e-04, 4.6310e-04],\n",
       "         [9.5252e-05, 5.6275e-05, 7.8580e-04, 6.9159e-05, 9.9847e-01,\n",
       "          4.9655e-05, 4.9471e-05, 4.2464e-05, 3.7907e-05, 3.6754e-05,\n",
       "          2.6315e-05, 8.8071e-05, 8.5371e-05, 5.0489e-05, 5.7913e-05],\n",
       "         [1.5457e-04, 1.8267e-04, 2.3885e-04, 3.1773e-04, 9.9538e-01,\n",
       "          5.6893e-05, 2.6456e-03, 1.1397e-04, 1.2315e-04, 1.5446e-04,\n",
       "          4.0066e-05, 1.7826e-04, 2.0417e-04, 1.0374e-04, 1.0107e-04],\n",
       "         [1.1336e-03, 3.1073e-04, 7.4842e-03, 1.0776e-03, 2.4128e-03,\n",
       "          3.4588e-04, 9.8111e-01, 4.8548e-04, 7.6333e-04, 1.0903e-03,\n",
       "          1.1960e-04, 2.0300e-03, 8.5102e-04, 3.1040e-04, 4.7747e-04],\n",
       "         [1.6746e-02, 2.2406e-03, 4.2815e-02, 9.2041e-03, 1.0854e-01,\n",
       "          1.1658e-03, 8.0248e-01, 9.5623e-04, 1.4089e-03, 1.5746e-03,\n",
       "          4.6724e-04, 3.7614e-03, 5.2747e-03, 2.3784e-03, 9.8492e-04],\n",
       "         [1.3181e-04, 5.2232e-05, 1.6100e-04, 2.2113e-05, 1.7785e-04,\n",
       "          9.1600e-05, 9.9902e-01, 4.7551e-05, 4.2694e-05, 6.3847e-05,\n",
       "          1.6659e-05, 4.2932e-05, 4.3741e-05, 4.9269e-05, 3.7577e-05],\n",
       "         [9.9699e-01, 1.5990e-04, 1.7865e-04, 2.9369e-04, 1.7684e-04,\n",
       "          2.8045e-04, 1.7803e-04, 5.6969e-05, 8.1260e-05, 2.0474e-04,\n",
       "          5.7936e-04, 3.8508e-04, 1.0479e-04, 1.4200e-04, 1.8436e-04],\n",
       "         [8.0676e-02, 8.5894e-02, 1.0997e-01, 6.1799e-02, 1.2071e-01,\n",
       "          5.2967e-02, 1.0740e-01, 4.4020e-02, 6.1881e-02, 5.3238e-02,\n",
       "          2.7221e-02, 2.9196e-02, 4.9164e-02, 6.3496e-02, 5.2365e-02]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pun_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of 'target_word' as the masked token: 5.441529538074974e-07\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import XLMRobertaForMaskedLM, XLMRobertaTokenizer\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"likhithasapu/gcm-xlmr-v2\"\n",
    "model = XLMRobertaForMaskedLM.from_pretrained(model_name)\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of 'यही' as the masked token: 0.10960280895233154\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set the input sentence with a masked token\n",
    "sentence = \"The goal of life is <mask>.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = tokenizer.encode(sentence, add_special_tokens=True, return_tensors=\"pt\")\n",
    "\n",
    "# Get the index of the masked token\n",
    "masked_index = torch.where(tokens == tokenizer.mask_token_id)[1]\n",
    "\n",
    "# Generate predictions for the masked token\n",
    "outputs = model(tokens)\n",
    "predictions = outputs.logits[0, masked_index, :]\n",
    "\n",
    "# Apply softmax to get probabilities\n",
    "probabilities = torch.softmax(predictions, dim=-1)\n",
    "\n",
    "# Get the probability of the target word\n",
    "target_word = \"यही\"\n",
    "target_word_id = tokenizer.encode(target_word, add_special_tokens=False)[0]\n",
    "target_probability = probabilities[0, target_word_id].item()\n",
    "\n",
    "print(f\"Probability of '{target_word}' as the masked token: {target_probability}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
